---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("student_alcohol/student-mat.csv").map(x => x.split(","))

scala> rdd.first
res1: Array[String] = Array(GP, F, 18, U, GT3, A, 4, 4, at_home, teacher, course, mother, 2, 2, 0, yes, no, no, no, yes, yes, no, no, 4, 3, 4, 1, 1, 3, 6, 5, 6, 6)

val categ_yesno = rdd.map(x => x(15)).distinct.zipWithIndex.collect.toMap
categ_yesno: scala.collection.immutable.Map[String,Long] = Map(yes -> 0, no -> 1)

val categ_sex = rdd.map(x => x(1)).distinct.zipWithIndex.collect.toMap
categ_sex: scala.collection.immutable.Map[String,Long] = Map(M -> 0, F -> 1)

val categ_address = rdd.map(x => x(3)).distinct.zipWithIndex.collect.toMap
categ_address: scala.collection.immutable.Map[String,Long] = Map(R -> 0, U -> 1)

val categ_famsize = rdd.map(x => x(4)).distinct.zipWithIndex.collect.toMap
cat_famsize: scala.collection.immutable.Map[String,Long] = Map(LE3 -> 0, GT3 -> 1)

val categ_pstatus = rdd.map(x => x(5)).distinct.zipWithIndex.collect.toMap
categ_pstatus: scala.collection.immutable.Map[String,Long] = Map(T -> 0, A -> 1)

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = Array.ofDim[Double](numCategories)
      categoryFeatures(categoryIdx) = 1.0
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,8,9,10,11)

concat.first
res1: Array[Double] = Array(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0)

val rdd1 = rdd.map( x => Array(categ_sex(x(1)),x(2),categ_address(x(3)),categ_famsize(x(4)),categ_pstatus(x(5)),x(6),x(7),x(12),x(13),x(14),categ_yesno(x(15)),categ_yesno(x(16)),categ_yesno(x(17)),categ_yesno(x(18)),categ_yesno(x(19)),categ_yesno(x(20)),categ_yesno(x(21)),categ_yesno(x(22)),x(23),x(24),x(25),x(28),x(29),x(30),x(31),x(32),x(26),x(27)))

rdd1.first
res2: Array[Any] = Array(1, 18, 1, 1, 1, 4, 4, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 4, 3, 4, 3, 6, 5, 6, 6, 1, 1)

val rdd2 = rdd1.map(x => x.map( y => y.toString.toDouble ))

rdd2.first
res3: Array[Double] = Array(1.0, 18.0, 1.0, 1.0, 1.0, 4.0, 4.0, 2.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 4.0, 3.0, 4.0, 3.0, 6.0, 5.0, 6.0, 6.0, 1.0, 1.0)

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.first
res4: Array[Double] = Array(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 18.0, 1.0, 1.0, 1.0, 4.0, 4.0, 2.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 4.0, 3.0, 4.0, 3.0, 6.0, 5.0, 6.0, 6.0, 1.0, 1.0)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(r => {
   val arr_size = r.size - 1
   val label = r(arr_size - 1).toDouble - 1
   val features = r.slice(0, arr_size - 2)
   LabeledPoint(label, Vectors.dense(features))
 })
 
data.cache

val sets = data.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)


---- MLlib Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(5).run(trainSet)

val validPredicts =  testSet.map(p => (model.predict(p.features),p.label))

validPredicts.take(20)
res60: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (4.0,0.0), (0.0,0.0), (0.0,2.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (2.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 51
validPredicts.count                            // 81
val accuracy = metrics.accuracy   // 0.6296296296296297

metrics.confusionMatrix
res63: org.apache.spark.mllib.linalg.Matrix =
50.0  4.0  3.0  0.0  2.0
12.0  1.0  2.0  0.0  0.0
2.0   0.0  0.0  0.0  1.0
1.0   0.0  1.0  0.0  0.0
0.0   0.0  1.0  1.0  0.0

---- MLlib Naive Bayes regression --------------

import org.apache.spark.mllib.classification.NaiveBayes
val model = NaiveBayes.train(trainSet)

val validPredicts =  testSet.map(p => (model.predict(p.features),p.label))

validPredicts.take(20)
res50: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,2.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 53
validPredicts.count                            // 81
val accuracy = metrics.accuracy   // 0.654320987654321

metrics.confusionMatrix
res67: org.apache.spark.mllib.linalg.Matrix =
52.0  4.0  1.0  1.0  1.0
12.0  1.0  1.0  1.0  0.0
3.0   0.0  0.0  0.0  0.0
1.0   0.0  1.0  0.0  0.0
0.0   1.0  0.0  1.0  0.0

----- MLlib DecisionTree regression --------------

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]()

val model = DecisionTree.trainClassifier(trainSet, 5, categoricalFeaturesInfo, "gini", 30, 32)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res68: Array[(Double, Double)] = Array((0.0,0.0), (2.0,0.0), (0.0,0.0), (2.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,2.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (2.0,0.0), (2.0,0.0), (0.0,1.0), (1.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 43
validPredicts.count                            // 81
val accuracy = metrics.accuracy   // 0.5308641975308642

metrics.confusionMatrix
res71: org.apache.spark.mllib.linalg.Matrix =
39.0  10.0  8.0  0.0  2.0
11.0  3.0   1.0  0.0  0.0
2.0   1.0   0.0  0.0  0.0
1.0   1.0   0.0  0.0  0.0
0.0   0.0   1.0  0.0  1.0
