---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("student_alcohol/student-mat.csv").map(x => x.split(","))

scala> rdd.first
res1: Array[String] = Array(GP, F, 18, U, GT3, A, 4, 4, at_home, teacher, course, mother, 2, 2, 0, yes, no, no, no, yes, yes, no, no, 4, 3, 4, 1, 1, 3, 6, 5, 6, 6)

val categ_yesno = rdd.map(x => x(15)).distinct.zipWithIndex.collect.toMap
categ_yesno: scala.collection.immutable.Map[String,Long] = Map(yes -> 0, no -> 1)

val categ_sex = rdd.map(x => x(1)).distinct.zipWithIndex.collect.toMap
categ_sex: scala.collection.immutable.Map[String,Long] = Map(M -> 0, F -> 1)

val categ_address = rdd.map(x => x(3)).distinct.zipWithIndex.collect.toMap
categ_address: scala.collection.immutable.Map[String,Long] = Map(R -> 0, U -> 1)

val categ_famsize = rdd.map(x => x(4)).distinct.zipWithIndex.collect.toMap
cat_famsize: scala.collection.immutable.Map[String,Long] = Map(LE3 -> 0, GT3 -> 1)

val categ_pstatus = rdd.map(x => x(5)).distinct.zipWithIndex.collect.toMap
categ_pstatus: scala.collection.immutable.Map[String,Long] = Map(T -> 0, A -> 1)

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = Array.ofDim[Double](numCategories)
      categoryFeatures(categoryIdx) = 1.0
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,8,9,10,11)

concat.first
res1: Array[Double] = Array(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0)

val rdd1 = rdd.map( x => Array(categ_sex(x(1)),x(2),categ_address(x(3)),categ_famsize(x(4)),categ_pstatus(x(5)),x(6),x(7),x(12),x(13),x(14),categ_yesno(x(15)),categ_yesno(x(16)),categ_yesno(x(17)),categ_yesno(x(18)),categ_yesno(x(19)),categ_yesno(x(20)),categ_yesno(x(21)),categ_yesno(x(22)),x(23),x(24),x(25),x(28),x(29),x(30),x(31),x(32),x(26),x(27)))

rdd1.first
res2: Array[Any] = Array(1, 18, 1, 1, 1, 4, 4, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 4, 3, 4, 3, 6, 5, 6, 6, 1, 1)

val rdd2 = rdd1.map(x => x.map( y => y.toString.toDouble ))

rdd2.first
res3: Array[Double] = Array(1.0, 18.0, 1.0, 1.0, 1.0, 4.0, 4.0, 2.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 4.0, 3.0, 4.0, 3.0, 6.0, 5.0, 6.0, 6.0, 1.0, 1.0)

val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.first
res4: Array[Double] = Array(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 18.0, 1.0, 1.0, 1.0, 4.0, 4.0, 2.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 4.0, 3.0, 4.0, 3.0, 6.0, 5.0, 6.0, 6.0, 1.0, 1.0)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(r => {
   val arr_size = r.size - 1
   val label = r(arr_size).toDouble - 1
   val features = r.slice(0, arr_size - 2)
   LabeledPoint(label, Vectors.dense(features))
 })
 
data.cache

val sets = data.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)


---- Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(5).run(trainSet)

val validPredicts =  testSet.map(p => (model.predict(p.features),p.label))

validPredicts.take(20)
res19: Array[(Double, Double)] = Array((0.0,1.0), (2.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,2.0), (1.0,1.0), (0.0,3.0), (0.0,3.0), (3.0,2.0), (0.0,0.0), (3.0,4.0), (3.0,3.0), (1.0,2.0), (0.0,2.0), (1.0,0.0), (3.0,4.0), (3.0,0.0), (0.0,0.0), (1.0,1.0), (2.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 25
validPredicts.count                            // 73
val accuracy = metrics.accuracy   // 0.3424657534246575

metrics.confusionMatrix
res22: org.apache.spark.mllib.linalg.Matrix =
15.0  3.0  3.0  2.0  1.0
7.0   5.0  2.0  2.0  2.0
8.0   3.0  2.0  2.0  1.0
3.0   1.0  1.0  1.0  1.0
0.0   1.0  1.0  4.0  2.0

---- MLlib Naive Bayes regression --------------

import org.apache.spark.mllib.classification.NaiveBayes
val model = NaiveBayes.train(trainSet)

val validPredicts =  testSet.map(p => (model.predict(p.features),p.label))

validPredicts.take(20)
res23: Array[(Double, Double)] = Array((0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,2.0), (0.0,1.0), (0.0,3.0), (0.0,3.0), (0.0,2.0), (0.0,0.0), (2.0,4.0), (3.0,3.0), (0.0,2.0), (0.0,2.0), (2.0,0.0), (3.0,4.0), (3.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 25
validPredicts.count                            // 73
val accuracy = metrics.accuracy   // 0.3424657534246575

metrics.confusionMatrix
res32: org.apache.spark.mllib.linalg.Matrix =
17.0  2.0  2.0  2.0  1.0
12.0  1.0  3.0  1.0  1.0
10.0  0.0  3.0  3.0  0.0
4.0   1.0  0.0  2.0  0.0
2.0   0.0  1.0  3.0  2.0

------------------------- Estimation is not so bad. But analyze the individual statistics and standardize 

scala> val vet1 = data.map{ case LabeledPoint(x,y) => y }
vet1: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[229] at map at <console>:37

scala> import org.apache.spark.mllib.linalg.distributed.RowMatrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

scala> val wineMat = new RowMatrix(vet1)
wineMat: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@18fcbbb2

scala> val wineStats = wineMat.computeColumnSummaryStatistics()
wineStats: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@201b7240

scala> wineStats.min
res28: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,3.0,0.0]

scala> wineStats.max
res29: org.apache.spark.mllib.linalg.Vector = [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,22.0,1.0,1.0,1.0,4.0,4.0,4.0,4.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,5.0,5.0,5.0,75.0,19.0,19.0]

scala> wineStats.mean
res30: org.apache.spark.mllib.linalg.Vector = [0.08607594936708861,0.14936708860759493,0.1468354430379747,0.2607594936708861,0.3569620253164557,0.04556962025316456,0.05063291139240506,0.07341772151898734,0.2810126582278481,0.549367088607595,0.2759493670886076,0.09113924050632911,0.3670886075949367,0.26582278481012656,0.6911392405063291,0.22784810126582278,0.0810126582278481,0.5265822784810127,16.69620253164557,0.7772151898734178,0.7113924050632912,0.10379746835443038,2.7493670886075945,2.5215189873417723,1.4481012658227845,2.0354430379746833,0.33417721518987337,0.8708860759493671,0.38734177215189874,0.5417721518987342,0.4911392405063291,0.20506329113924052,0.05063291139240506,0.1670886075949367,0.6658227848101266,3.9443037974683564,3.235443037974683,3.1088607594936697,3.5544303797468384...

scala> wineStats.variance
res31: org.apache.spark.mllib.linalg.Vector = [0.07886654244040352,0.12737904003084238,0.1255927520400951,0.19325322881192572,0.23012272698065928,0.04360341836406862,0.04819122277195914,0.06820021846687657,0.20255734755509866,0.24819122277195915,0.20030842382574054,0.08304311508063997,0.2329242433978025,0.19565636445415407,0.21400758208571613,0.17637987534537042,0.0746385658292103,0.24992610679174965,1.6282850350189506,0.17359120992096638,0.20583435070359185,0.09325965430829532,1.1984450298785574,1.1841804279380586,0.48651288312022095,0.7043243590567373,0.5530167705455246,0.11272890830816681,0.23791042858060785,0.24888517637987534,0.25055580543596995,0.1634260746642678,0.04819122277195914,0.13952322816937607,0.22306753196684445,0.8039966587418876,0.9977253742851631,1.2393882927456141,1....


-------------------------- Decide to scale features because columns have different scales and then retrain the model 

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, true).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))
val validScaled = testSet.map(x => LabeledPoint(x.label, scaler.transform(x.features)))

---- Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(5).run(trainScaled)

val validPredicts =  validScaled.map(p => (model.predict(p.features),p.label))

validPredicts.take(20)
res34: Array[(Double, Double)] = Array((0.0,1.0), (2.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,2.0), (1.0,1.0), (0.0,3.0), (0.0,3.0), (3.0,2.0), (0.0,0.0), (3.0,4.0), (2.0,3.0), (4.0,2.0), (3.0,2.0), (3.0,0.0), (3.0,4.0), (3.0,0.0), (3.0,0.0), (1.0,1.0), (2.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 21
validPredicts.count                            // 73
val accuracy = metrics.accuracy   // 0.2876712328767123

metrics.confusionMatrix
res37: org.apache.spark.mllib.linalg.Matrix =
10.0  4.0  5.0  4.0  1.0
4.0   5.0  4.0  3.0  2.0
5.0   2.0  2.0  4.0  3.0
2.0   2.0  2.0  1.0  0.0
0.0   1.0  1.0  3.0  3.0

------- Not helpful on standardizing. Using decision tree model to evaluate performance.

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]()

val model = DecisionTree.trainClassifier(trainSet, 5, categoricalFeaturesInfo, "gini", 30, 32)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res38: Array[(Double, Double)] = Array((0.0,1.0), (3.0,0.0), (0.0,0.0), (1.0,1.0), (1.0,2.0), (0.0,1.0), (1.0,3.0), (0.0,3.0), (0.0,2.0), (0.0,0.0), (2.0,4.0), (0.0,3.0), (0.0,2.0), (2.0,2.0), (1.0,0.0), (3.0,4.0), (0.0,0.0), (1.0,0.0), (1.0,1.0), (0.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 25
validPredicts.count                            // 73
val accuracy = metrics.accuracy   // 0.3424657534246575

scala> metrics.confusionMatrix
res41: org.apache.spark.mllib.linalg.Matrix =
11.0  5.0  4.0  4.0  0.0
7.0   8.0  0.0  2.0  1.0
6.0   6.0  3.0  0.0  1.0
3.0   2.0  1.0  1.0  0.0
2.0   0.0  2.0  2.0  2.0

